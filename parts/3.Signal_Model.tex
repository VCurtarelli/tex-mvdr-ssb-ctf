\section{Signal and Array Model}
\label{sec:signal_model}

Let there be a sensor array of any shape, which is comprised of $M$ sensors, within a reverberant environment. This environment also contains a desired and an interfering sources (name $x[n]$ and $v[n]$), and also uncorrelated noise, $r_m[n]$ (at each sensor $m$).

We denote $h_m[n]$ as the room impulse response, between the desired signal (at source) and the $m$-th sensor. We similarly define $g_m[n]$ for the interfering signal at source. From this, we write $y_m[n]$ as the observed signal at the $m$-th sensor as
\begin{equation}
	\label{eq:sec2:time_model_basic}
	y_m[n] = h_m[n] \ast x[n] + g_m[n] \ast v[n] + r_m[n]
\end{equation}

We let $m'$ be the reference sensor's index (without compromise, $m' = 1$). We let $x_1[n] = h_1[n] \ast x[n]$ (and similarly for $v_1[n]$). $b_m[n]$ is the \textit{relative} impulse response between the desired signal (at the reference sensor) and the $m$-th sensor, such that
\begin{equation}
	b_m[n] \ast x_1[n] = h_m[n] \ast x[n]
\end{equation}
and we similarly define $c_m[n]$ such that $c_m[n] \ast v_1[n] = g_m[n] \ast v[n]$. Therefore, \cref{eq:sec2:time_model_basic} becomes
\begin{equation}
	\label{eq:sec2:time_model_relative}
	y_m[n] = b_m[n] \ast x_1[n] + c_m[n] \ast v_1[n] + r_m[n]
\end{equation}

We can use the CTF model \cite{ctf_model} along a time-frequency transform to turn \cref{eq:sec2:time_model_relative} into
\begin{equation}
	\label{eq:sec2:time-freq_model_conv}
	Y_m[l,k] = B_m[l,k] \ast X_1[l,k] + C_m[l,k] \ast V_1[l,k] + R_m[l,k]
\end{equation}

where $Y_m[l,k]$ is the transform of $y_m[n]$ (resp. $B_m[l,k]$, $X_1[l,k]$, $C_m[l,k]$, $V_1[l,k]$ and $R_m[l,k]$); $l$ is the window index, and $k$ the bin index, with $0 \leq k \leq K-1$; and the convolution is in the window-index axis.

Assuming that $B_m[l,k]$ is a finite (possibly truncated) response with $L_B$ windows, then
\begin{equation}
	B_m[l,k] \ast X_1[l,k] = \tr{\bvb{m}}[k] \bvx{1}[l,k]
\end{equation}
in which
\begin{subalign}
	\bvb{m}[k] & = \tr{\tup{ B_m[0,k] , B_m[1,k] , , B_m[L_B-1,k] }} \\
	\bvx{1}[l,k] & = \tr{\tup{ B_m[l,k] , B_m[l-1,k] , , B_m[l-L_B+1,k] }}
\end{subalign}
and similarly we define $\bvc{m}[k]$ and $\bvv{1}[l,k]$. Note that $\bvb{m}[k]$ doesn't depend on the index $l$, since the system is time-invariant. With this, \cref{eq:sec2:time-freq_model_conv} becomes
\begin{equation}
	\label{eq:sec2:time-freq_model_mult1}
	Y_m[l,k] = \tr{\bvb{m}}[k] \bvx{1}[l,k] + \tr{\bvc{m}}[k] \bvv{1}[l,k] + R_m[l,k]
\end{equation}

Vectorizing the signals sensor-wise, we finally get
\begin{equation}
	\bvy[l,k] = \tr{\bvB}[k] \bvx{1}[l,k] + \tr{\bvC}[k] \bvv{1}[l,k] + \bvr[l,k]
\end{equation}
where
\begin{equation}
	\bvy[l,k] = \tr{\tup{ y_1[l,k],,y_M[l,k] }}
\end{equation}
and similarly for the other variables. In this situation, $\bvB[k]$ and $\bvC[k]$ are $\sz{L_B}{M}$ and $\sz{L_C}{M}$ matrices respectively; $\bvx{1}[l,k]$ and $\bvv{1}[l,k]$ are $\sz{L_B}{1}$ and $\sz{L_C}{1}$ vectors respectively; and $\bvy[l,k]$ and $\bvr[l,k]$ are $\sz{M}{1}$ vectors.

\subsection{Reverb-aware formulation}
We define $\Delta$ as the window-index in which $b_1[n]$ starts, and assume that $b_1[n]$ starts at the start of a window of the transform\footnote{This can be easily achieved by left zero-padding all $b_m[n]$ appropriately.}. We assume that the first window of $\bvB[k]$ is the desired part of speech, and the rest is an undesired component, which is only reverberation.

With this, we write
\begin{equation}
	\tr{\bvB}[k] \bvx{1}[l,k] = \bvd{x}[k] X_1[l,k] + \sum_{\substack{l'=0 \\ l' \neq \Delta}}^{L_B-1} \bvp{B,l'}[k] X_1[l-l',k]
\end{equation}
where $\bvd{x}[k]$ is the $k$-th row of $\bvB[k]$, and $\bvp{B,l'}[k]$ is the $l'$-th row of $\bvB[k]$. With this, $\bvd{x}[k] X_1[l,k]$ is the desired speech component of $\tr{\bvB}[k] \bvx{1}[l,k]$, and the summation over $l'$ is the undesired component.

We define $\bvp{C,l''}$ similarly, such that
\begin{equation}
	\tr{\bvC}[k] \bvv{1}[l,k] = \sum_{l''=0}^{L_C-1} \bvp{C,l''}[k] V_1[l-l'',k]
\end{equation}

From here, we can write
\begin{equation}
	\bvy[l,k] = \bvd{x}[k] X_1[l,k] + \bvw[l,k]
\end{equation}
with $\bvw[l,k]$ being the undesired signal (undesired speech components + interfering source + noise), given by
\begin{equation}
	\bvw[l,k] = \sum_{\substack{l'=0 \\ l' \neq \Delta}}^{L_B-1} \bvp{B,l'}[k] X_1[l-l',k] + \sum_{l''=0}^{L_C-1}\bvp{C,l''}[k] V_1[l-l'',k] + \bvr[l,k]
\end{equation}

\subsection{MVDR beamformer}

We use an LTI filter $\bvf[l,k]$ to estimate the desired signal at the reference sensor, such that
\begin{equations}
	Z[l,k]
	& = \he{\bvf}[l,k] \bvy[l,k] \\
	& \approx X_1[l,k]
\end{equations}

In order to minimize the undesired signal $\bvw[l,k]$, we will use an MVDR beamformer \cite{mvdr-beamformer}, whose formulation is
\begin{equation}
	\label{eq:sec2:minimization_problem_mvdr}
	\bvf^{\star}[l,k] = \min_{\bvf[l,k]} \he{\bvf[l,k]} \Corr{\bvw}[l,k] \bvf[l,k]~\text{s.t.}~\he{\bvf}[l,k] \bvd{x}[k] = 1
\end{equation}
in which $\he{\bvf}[l,k] \bvd{x}[k] = 1$ is the distortionless constraint, and $\Corr{\bvw}[l,k]$ is the correlation matrix of the undesired signal, given by
\begin{equations}
	\Corr{\bvw}[l,k] 
	& = \sum_{\substack{l'=0 \\ l' \neq \Delta}}^{L_B-1} \he{\bvp{B,l'}}[k] \bvp{B,l'}[k] \var{X_1}[l-l',k] \\
	& + \sum_{l''=0}^{L_C-1} \he{\bvp{C,l''}}[k] \bvp{C,l''}[k] \var{V_1}[l-l'',k] \\
	& + \id{M} \var{R}[l,k]
\end{equations}
where $\var{X_1}[l,k]$ is the variance of $X_1[l,k]$ (same for $\var{V_1}[l,k]$), and $\id{M}$ is the $\sz{M}{M}$ identity matrix, assuming that the distribution of $\bvr[l,k]$ is the same for all sensors.

Even though when windowing the signal for the time-frequency transform, we use overlapping windows, here we assume that $X_1[l_1,k]$ is independent of $X_1[l_2,k]$, for simplicity.

The solution to \cref{eq:sec2:minimization_problem_mvdr} is given by
\begin{equation}
	\bvf{\mvdr}[l,k] = \frac{ \inv{\Corr{\bvw}}[l,k] \bvd{x}[l,k] }{ \he{\bvd{x}}[l,k] \inv{\Corr{\bvw}}[l,k] \bvd{x}[l,k] }
\end{equation}


%Let $S$ be a uniform rectangular array (URA) of sensors over the ${\x}-{\y}$ plane in an anechoic environment with desired and undesired sources. The URA comprises $\Mx$ sensors spaced $\dx$ apart along the $\x$-axis and $\My$ sensors spaced $\dy$ apart along the $\y$-axis, resulting in a total of $M = \Mx\My$ sensors. Assume a source in the far-field on the same plane as the sensor array (that is, with elevation $\phi = 0\dg$), impinging on it from an azimuth angle $\t$. As it is unusual, in speech enhancement, for the desired and undesired sources to have the same azimuth, differing only by elevation, we assume the elevation to be $0\dg$. This constraint can be easily removed without affecting the mathematical framework developed.
%
%Let $\bvD(\w,\t)$ denote the steering matrix of size $\sz{\Mx}{\My}$ with elements $\{m_x,m_y\}$ given by
%\begin{equation}
%	\el{\bvD(\w,\t)}[m_x,m_y] = \exp*{-\j \tfrac{\w}{c}\R[m_x,m_y] \cos(\t-\p[m_x,m_y])} \eqc
%\end{equation}
%where $c=340~$m/s is the speed of sound,
%$(\R[m_x,m_y],\p[m_x,m_y])$ are the polar  coordinates of the sensor at $(m_x\dx,m_y\dy)$, $\w = 2\pi f$ is the angular frequency, 
%$f$ is the temporal frequency, and $\j=\Sqrt{-1}$ is the imaginary unit. We denote by $\bvd(\w,\t) = \vect{\bvD(\w,\t)}$ the $\sz{M}{1}$ steering vector, with $M=M_xM_y$, and $\vect{\,\cdot\,}$ being the vectorization operation; and let $\bvD(\w,\t) = \ivect{\My}{\bvd(\w,\t)}$ denote the inverse vectorization of $\bvd(\w,\t)$.
%
%The observed signal vector $\bvy(\w)$, of size $M\times 1$, for all sensors in the frequency domain can be written as
%\begin{equation}
%	\bvy(\w) = \bvd(\w,\td) X(\w) + \bvv(\w) \eqc
%\end{equation}
%where $X(\w)$ is the desired signal at the reference sensor, $\bvd(\w,\td)$ is the steering vector of the desired source from the direction $\td$, and $\bvv(\w)$ is the additive noise signal vector. All signals are assumed to be zero-mean and uncorrelated. We can estimate the desires signal $X(\w)$ as $Z(\w)$ using a beamformer $\bvh(\w)$ (assumed a 2-D beamformer), through the linear filtering
%\begin{equation}
%	Z(\w) = \he{\bvh(\w)} \bvy(\w) \eqc
%\end{equation}
%where the superscript $\he{}$ denotes the conjugate-transpose operator. The beamformer is called distortionless if it satisfies $\he{\bvh(\w)} \bvd(\w,\td) = 1$ for all $\w$. In that case, 
%\begin{equation}
%    \label{eq:Zw_beamformer_output}
%	Z(\w) = X(\w) + \bvv{\rn}(\w) \eqc
%\end{equation}
%where $\bvv{\rn}(\w)=\he{\bvh(\w)} \bvv(\w)$ is the residual noise at the beamformer's output. This constraint guarantees the beamformer to not affect the desired signal, only altering the undesired noise signal.
%
%From here on, $\w$ will be omitted unless in definitions, and $\t$ in the steering vectors will appear in subscripts where necessary. When no angle is shown, $\bvd=\bvd(\w,\td)$ is assumed to be the desired-signal steering vector for conciseness.
%
%\subsection{Beamformer metrics}
%
%The beampattern $\beam$, as a function of the beamformer $\bvh$ and the direction $\t$ (through the steering vector $\bvd{\t}$) is given by
%\begin{equation}
%    \beam{\bvh,\bvd{\t}} = \he{\bvh} \bvd{\t} \eqp
%\end{equation}
%
%Given the desired-signal steering vector $\bvd$, the white noise gain (WNG), desired signal distortion index (DSDI), and directivity factor (DF) are, respectively
%\begin{subgather}
%	\wng{\bvh,\bvd} = \frac{ \abs{ \he{\bvh}\bvd }^2 }{ \he{\bvh}\bvh } \eqc \\
%	\dsdi{\bvh,\bvd} = \abs{ \he{\bvh} \bvd - 1 }^2 \eqc \\
%	\df{\bvh,\bvd} = \frac{ \abs{ \he{\bvh} \bvd }^2 }{ \he{\bvh} \bv{\Gamma} \bvh } \label{subeq:def_df} \eqc
%\end{subgather}
%where $\bv{\Gamma}(\w)$ is the spherical isotropic noise field coherence matrix \cite{habets_generating_2007}. Using the DSDI, the distortionless constraint can also be written as $\dsdi{\bvh,\bvd} = 0$. 